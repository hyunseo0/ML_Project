{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, AffinityPropagation, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "from pyclustering.cluster.clarans import clarans as Clarans\n",
    "\n",
    "\n",
    "class ScalerOptions:\n",
    "    def __init__(self, attributes: list[str] = [], scalers: list[str] = [\"\"]):\n",
    "        self.attributes: list[str] = attributes\n",
    "        self.scalers: list[str] = [name.lower() for name in scalers]\n",
    "\n",
    "\n",
    "class EncoderOptions:\n",
    "    def __init__(self, attributes: list[str] = [], encoders: list[str] = [\"\"]):\n",
    "        self.attributes: list[str] = attributes\n",
    "        self.encoders: list[str] = [name.lower() for name in encoders]\n",
    "\n",
    "\n",
    "class ModelOption:\n",
    "    def __init__(self, model_name: str, options: Dict[str, Any] = {}):\n",
    "        self.model_name: str = model_name.lower()\n",
    "        self.options: Dict[str, Any] = options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import axhline\n",
    "\n",
    "\n",
    "def kmeans(dataset: pd.DataFrame, options: Dict[str, Any] = {}, k: int = 1):\n",
    "    \"\"\"\n",
    "    K-Means Clustering\n",
    "    return list of results : list[Results]\n",
    "    # Parameters\n",
    "    dataset: dataset\n",
    "    options: options dictionaly for model\n",
    "    k: number of clusters\n",
    "    \"\"\"\n",
    "    n_init = options[\"n_init\"] if \"n_init\" in options else 3\n",
    "    max_iter = options[\"max_iter\"] if \"max_iter\" in options else 300\n",
    "    tol = options[\"tol\"] if \"tol\" in options else 1e-4\n",
    "    verbose = options[\"verbose\"] if \"verbose\" in options else 0\n",
    "    random_state = options[\"random_state\"] if \"random_state\" in options else None\n",
    "    best_predicted = None\n",
    "    model = KMeans(\n",
    "        n_clusters=k,\n",
    "        n_init=n_init,\n",
    "        max_iter=max_iter,\n",
    "        tol=tol,\n",
    "        verbose=verbose,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    predicted = model.fit_predict(dataset)\n",
    "    _, sizes = np.unique(predicted, return_counts=True)\n",
    "    silhouette = silhouette_score(dataset, predicted)\n",
    "    result = {\n",
    "        \"predicted\": predicted,\n",
    "        \"k\": k,\n",
    "        \"ninti\": n_init,\n",
    "        \"max_iter\": max_iter,\n",
    "        \"tol\": tol,\n",
    "        \"verbose\": verbose,\n",
    "        \"silhouette\": silhouette,\n",
    "        \"centroids\": pd.DataFrame(\n",
    "            model.cluster_centers_,\n",
    "        ),\n",
    "        \"random_state\": random_state,\n",
    "        \"sizes\": sizes,\n",
    "    }\n",
    "\n",
    "    return [result]\n",
    "\n",
    "\n",
    "def gmm(dataset: pd.DataFrame, options: Dict[str, Any] = {}, k: int = 1):\n",
    "    \"\"\"\n",
    "    Gaussian Mixture Clustring.\n",
    "    return list of results : list[Results]\n",
    "    # Parameters\n",
    "    dataset: dataset\n",
    "    options: options dictionary for model\n",
    "        covariance_types :list[str]\n",
    "    k: number of clusters\n",
    "    \"\"\"\n",
    "    covariance_types = (\n",
    "        options[\"covariance_types\"] if \"covariance_types\" in options else [\"full\"]\n",
    "    )\n",
    "    tol = options[\"tol\"] if \"tol\" in options else 1e-3\n",
    "    max_iter = options[\"max_iters\"] if \"max_iters\" in options else 100\n",
    "    n_init = options[\"n_inits\"] if \"n_inits\" in options else 1\n",
    "    random_state = options[\"random_state\"] if \"random_state\" in options else None\n",
    "    verbose = options[\"verbose\"] if \"verbose\" in options else 0\n",
    "    results = []\n",
    "    for covariance_type in covariance_types:\n",
    "        model = GaussianMixture(\n",
    "            n_components=k,\n",
    "            covariance_type=covariance_type,\n",
    "            max_iter=max_iter,\n",
    "            tol=tol,\n",
    "            verbose=verbose,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        predicted = model.fit_predict(dataset)\n",
    "        _, sizes = np.unique(predicted, return_counts=True)\n",
    "\n",
    "        result = {\n",
    "            \"predicted\": predicted,\n",
    "            \"k\": k,\n",
    "            \"ninti\": n_init,\n",
    "            \"max_iter\": max_iter,\n",
    "            \"tol\": tol,\n",
    "            \"verbose\": verbose,\n",
    "            \"random_state\": random_state,\n",
    "            \"mean\": model.means_,\n",
    "            \"covariances\": model.covariances_,\n",
    "            \"sizes\": sizes,\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def clarans(dataset: pd.DataFrame, options: Dict[str, Any] = {}, k: int = 1):\n",
    "    \"\"\"\n",
    "    Clarans Clustring.\n",
    "    return list of results : list[Results]\n",
    "    # Parameters\n",
    "    dataset: dataset\n",
    "    options: options for model\n",
    "        numlocals: The number of local minima obtained\n",
    "        maxneighbors: The maximum number of neighbors examined\n",
    "    k: number of clusters\n",
    "    \"\"\"\n",
    "    numlocals = options[\"numlocals\"] if \"numlocals\" in options else [3]\n",
    "    maxneighbors = options[\"maxneighbors\"] if \"maxneighbors\" in options else [3]\n",
    "\n",
    "    results = []\n",
    "    for numlocal in numlocals:\n",
    "        for maxneighbor in maxneighbors:\n",
    "            model = Clarans(dataset.values, k, numlocal, maxneighbor)\n",
    "            model.process()\n",
    "            clusters = model.get_clusters()\n",
    "            medoids = model.get_medoids()\n",
    "            # labeling\n",
    "            print(dataset.values.shape)\n",
    "            predicted = np.zeros(dataset.values.shape[0])\n",
    "            sizes = []\n",
    "            for idx, cluster in enumerate(clusters):\n",
    "                predicted[cluster] = idx\n",
    "                sizes.append(len(cluster))\n",
    "            results.append(\n",
    "                {\n",
    "                    \"numlocals\": numlocals,\n",
    "                    \"maxneighbor\": maxneighbor,\n",
    "                    \"medoids\": medoids,\n",
    "                    \"predicted\": predicted,\n",
    "                    \"k\": k,\n",
    "                    \"sizes\": sizes,\n",
    "                }\n",
    "            )\n",
    "    return results\n",
    "\n",
    "\n",
    "def dbscan(dataset: pd.DataFrame, options: Dict[str, Any] = {}, k: int = 1):\n",
    "    \"\"\"\n",
    "    dbscan Clustring.\n",
    "    return list of results : list[Results]\n",
    "    # Parameters\n",
    "    dataset: dataset\n",
    "    options: options for model\n",
    "        min_samples:list[int],  The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "        eps:list[float],   The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "        algorithms:list[str]    The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors\n",
    "        metrics:list[str]   The metric to use when calculating distance between instances in a feature array\n",
    "    k: number of clusters\n",
    "    \"\"\"\n",
    "    eps_list = options[\"eps\"] if \"eps\" in options else [0.8]\n",
    "    min_samples = options[\"min_samples\"] if \"min_samples\" in options else [3]\n",
    "    algorithms = options[\"algorithms\"] if \"algorithms\" in options else [\"auto\"]\n",
    "    metrics = options[\"metrics\"] if \"metrics\" in options else [\"euclidean\"]\n",
    "    results = []\n",
    "    for eps in eps_list:\n",
    "        for min_sample in min_samples:\n",
    "            for algorithm in algorithms:\n",
    "                for metric in metrics:\n",
    "                    model = DBSCAN(\n",
    "                        eps=eps,\n",
    "                        min_samples=min_sample,\n",
    "                        metric=metric,\n",
    "                        algorithm=algorithm,\n",
    "                    )\n",
    "                    predicted = model.fit_predict(dataset)\n",
    "                    _, sizes = np.unique(predicted, return_counts=True)\n",
    "                    result = {\n",
    "                        \"predicted\": predicted,\n",
    "                        \"k\": k,\n",
    "                        \"eps\": eps,\n",
    "                        \"min_sample\": min_sample,\n",
    "                        \"algorithm\": algorithm,\n",
    "                        \"sizes\": sizes,\n",
    "                    }\n",
    "                    results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def affinity(dataset, options):\n",
    "    \"\"\"\n",
    "    dbscan Clustring.\n",
    "    return list of results : list[Results]\n",
    "    # Parameters\n",
    "    dataset: dataset\n",
    "    options: options for model\n",
    "        convergence_iters:list[int],   Number of iterations with no change in the number of estimated clusters that stops the convergence.\n",
    "        affinities:list[str]   Which affinity to use. At the moment ‘precomputed’ and euclidean are supported. ‘euclidean’ uses the negative squared euclidean distance between points.\n",
    "    \"\"\"\n",
    "    convergence_iters = (\n",
    "        options[\"convergence_iters\"] if \"convergence_iters\" in options else [15]\n",
    "    )\n",
    "    max_iter = options[\"max_iter\"] if \"max_iter\" in options else 200\n",
    "    affinities = options[\"affinities\"] if \"affinities\" in options else [\"euclidean\"]\n",
    "\n",
    "    verbose = options[\"verbose\"] if \"verbose\" in options else 0\n",
    "    random_state = options[\"random_state\"] if \"random_state\" in options else None\n",
    "    results = []\n",
    "    for convergence_iter in convergence_iters:\n",
    "        for affinity in affinities:\n",
    "            model = AffinityPropagation(\n",
    "                max_iter=max_iter,\n",
    "                verbose=verbose,\n",
    "                random_state=random_state,\n",
    "                affinity=affinity,\n",
    "                convergence_iter=convergence_iter,\n",
    "            )\n",
    "            predicted = model.fit_predict(dataset)\n",
    "            silhouette = silhouette_score(dataset, predicted)\n",
    "            _, sizes = np.unique(predicted, return_counts=True)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"max_iter\": max_iter,\n",
    "                    \"verbose\": verbose,\n",
    "                    \"convergence_iter\": convergence_iter,\n",
    "                    \"affinity\": affinity,\n",
    "                    \"k\": len(model.cluster_centers_indices_),\n",
    "                    \"silhouette\": silhouette,\n",
    "                    \"predicted\": predicted,\n",
    "                    \"sizes\": sizes,\n",
    "                    \"random_state\": random_state,\n",
    "                }\n",
    "            )\n",
    "    return results\n",
    "\n",
    "\n",
    "def AutoML(\n",
    "    X: pd.DataFrame,\n",
    "    Y: pd.DataFrame,\n",
    "    scale_option: ScalerOptions,\n",
    "    encode_option: EncoderOptions,\n",
    "    mode_list: list[ModelOption],\n",
    "    k_list: list[int],\n",
    "):\n",
    "    def evaluate(evaluttes, y_custer):\n",
    "        for evalutte in evaluttes:\n",
    "            score = np.sum(np.power(evalutte[\"sizes\"] - y_custer[\"sizes\"], 2))\n",
    "            evalutte[\"score\"] = score\n",
    "        return evaluttes\n",
    "\n",
    "    def predict_by_y(y, k):\n",
    "        ret = kmeans(y, k=k)[0]\n",
    "        return {\"predicted\": ret[\"predicted\"], \"sizes\": ret[\"sizes\"]}\n",
    "\n",
    "    def scale(scaler_name: str, attrs: list[str], dataset: pd.DataFrame):\n",
    "        scaler_name = scaler_name.split(\"scaler\")[0]\n",
    "        if scaler_name == \"\":\n",
    "            return dataset.copy()\n",
    "        elif scaler_name == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "            pass\n",
    "        elif scaler_name == \"robust\":\n",
    "            scaler = RobustScaler()\n",
    "            pass\n",
    "        elif scaler_name == \"minmax\":\n",
    "            scaler = MinMaxScaler()\n",
    "            pass\n",
    "        elif scaler_name == \"maxabs\":\n",
    "            scaler = MaxAbsScaler()\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Not Defined Scaler\")\n",
    "        new_data = dataset.copy()\n",
    "        new_data[attrs] = scaler.fit_transform(dataset[attrs])\n",
    "        return new_data\n",
    "\n",
    "    def encode(encoder_name: str, attrs: list[str], dataset: pd.DataFrame):\n",
    "        encoder_name = encoder_name.split(\"encoder\")[0]\n",
    "        if encoder_name == \"\":\n",
    "            return dataset.copy()\n",
    "        copied = dataset.copy()\n",
    "        if encoder_name == \"onehot\":\n",
    "            encoder = OneHotEncoder()\n",
    "            copied.drop(columns=attrs)\n",
    "            encoded = pd.get_dummies(dataset[attrs])\n",
    "            copied = pd.concat([copied, encoded], axis=1)\n",
    "\n",
    "        elif encoder_name == \"ordinal\":\n",
    "            encoder = OrdinalEncoder()\n",
    "            copied[attrs] = encoder.fit_transform(dataset[attrs])\n",
    "        else:\n",
    "            raise Exception(\"Not Define Encoder\")\n",
    "        return copied\n",
    "\n",
    "    all_results = []\n",
    "    for scaler in scale_option.scalers:\n",
    "        scaled = scale(scaler, scale_option.attributes, X)\n",
    "        for encoder in encode_option.encoders:\n",
    "            encoded = encode(encoder, encode_option.attributes, scaled)\n",
    "            aff_modelList=[model for model in mode_list if model.model_name in ['affinity','affinitypropagation']]\n",
    "            for model in aff_modelList:\n",
    "                results = affinity(encoded, model.options)\n",
    "                for result in results:\n",
    "                    print(type(result['sizes']))\n",
    "                    y_cluster = predict_by_y(Y, len(result['sizes']))\n",
    "                    result = evaluate ([result], y_cluster)\n",
    "                    all_results.extend(result)\n",
    "\n",
    "            for k in k_list:\n",
    "                y_cluster = predict_by_y(Y, k)\n",
    "                for model in mode_list:\n",
    "                    if model.model_name == \"kmeans\":\n",
    "                        results = kmeans(encoded, model.options, k)\n",
    "\n",
    "                    elif model.model_name == \"gmm\":\n",
    "                        results = gmm(encoded, model.options, k)\n",
    "                    elif model.model_name == \"clarans\":\n",
    "                        results = clarans(encoded, model.options, k)\n",
    "                    elif model.model_name == \"dbscan\":\n",
    "                        results = dbscan(encoded, model.options, k)\n",
    "                        print(results)\n",
    "                    elif (\n",
    "                        model.model_name == \"affinity\"\n",
    "                        or model.model_name == \"affinitypropagation\"\n",
    "                    ):\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise Exception(\"Not define Model\")\n",
    "                    result = evaluate(results, y_cluster)\n",
    "                    all_results.extend(result)\n",
    "                \n",
    "            \n",
    "    return all_results\n",
    "\n",
    "def load_data():\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "    df = pd.read_csv(\"housing.csv\")\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    y = df.loc[:, [\"median_house_value\"]]\n",
    "    x = df.loc[\n",
    "        :,\n",
    "        [\n",
    "            \"longitude\",\n",
    "            \"latitude\",\n",
    "            \"housing_median_age\",\n",
    "            \"total_rooms\",\n",
    "            \"total_bedrooms\",\n",
    "            \"population\",\n",
    "            \"households\",\n",
    "            \"median_income\",\n",
    "            \"ocean_proximity\",\n",
    "        ],\n",
    "    ]\n",
    "    x2 = df.iloc[:,[0]]\n",
    "    x1 = df.sample(n=2, axis=1)\n",
    "    copied = pd.concat([x1, x2], axis=1)\n",
    "    print(copied)\n",
    "\n",
    "    # set columns sets\n",
    "    numerical_columns = [\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"housing_median_age\",\n",
    "        \"total_rooms\",\n",
    "        \"total_bedrooms\",\n",
    "        \"population\",\n",
    "        \"households\",\n",
    "        \"median_income\",\n",
    "    ]\n",
    "    categorical_columns = [\"ocean_proximity\"]\n",
    "    return x, y, numerical_columns, categorical_columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    x, y, numericals, categoricals = load_data()\n",
    "    scale_option = ScalerOptions(\n",
    "        attributes=numericals,\n",
    "        scalers=[\"standardscaler\", \"robustscaler\", \"minmaxscaler\", \"maxabsscaler\"],\n",
    "    )\n",
    "    encode_option = EncoderOptions(\n",
    "        attributes=categoricals, encoders=[\"onehot\", \"ordinal\"]\n",
    "    )\n",
    "    # K-means, EM(GMM), CLARANS, DBSCAN, Affinity Propagation\n",
    "    k_means = ModelOption(model_name=\"kmeans\")\n",
    "    gmm = ModelOption(model_name=\"gmm\")\n",
    "    clarans = ModelOption(model_name=\"clarans\")\n",
    "    dbscan = ModelOption(\"dbscan\")\n",
    "    affinity = ModelOption(\"affinity\")\n",
    "    for i in range(5):\n",
    "        result1 = AutoML(\n",
    "            x.iloc[:100, :],\n",
    "            y.iloc[:100],\n",
    "            ScalerOptions(numericals, [\"standard\"]),\n",
    "            EncoderOptions(categoricals, [\"ordinal\"]),\n",
    "            [\n",
    "                k_means,\n",
    "                # clarans,\n",
    "                # dbscan,\n",
    "                affinity,\n",
    "            ],\n",
    "            k_list=[3],\n",
    "        )\n",
    "        print(result1)\n",
    "        print(type(result1))\n",
    "    # AutoML(\n",
    "    #     x,\n",
    "    #     y,\n",
    "    #     scale_option,\n",
    "    #     encode_option,\n",
    "    #     [k_means, gmm, clarans, dbscan, affinity],\n",
    "    #     k_list=[3, 4, 5],\n",
    "    # )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       population  housing_median_age  index\n",
      "0           322.0                41.0      0\n",
      "1          2401.0                21.0      1\n",
      "2           496.0                52.0      2\n",
      "3           558.0                52.0      3\n",
      "4           565.0                52.0      4\n",
      "...           ...                 ...    ...\n",
      "20428       845.0                25.0  20635\n",
      "20429       356.0                18.0  20636\n",
      "20430      1007.0                17.0  20637\n",
      "20431       741.0                18.0  20638\n",
      "20432      1387.0                16.0  20639\n",
      "\n",
      "[20433 rows x 3 columns]\n",
      "<class 'numpy.ndarray'>\n",
      "[{'max_iter': 200, 'verbose': 0, 'convergence_iter': 15, 'affinity': 'euclidean', 'k': 13, 'silhouette': 0.23085639527322566, 'predicted': array([ 1,  0,  1,  1,  5,  5,  2,  2,  2,  2,  3,  2,  2,  5,  2,  5,  3,\n",
      "        3,  3,  5,  4,  3,  3,  3,  3,  4,  4,  3,  3,  4,  3,  3,  3,  4,\n",
      "        3,  4,  3,  3,  5,  2,  5,  4,  4,  5,  5,  5,  5,  4,  4,  4,  9,\n",
      "        3, 12,  3,  4,  4,  4,  4,  4,  9,  7,  6,  7,  7,  7,  7,  7,  7,\n",
      "        7,  8,  9,  7,  7, 10,  9,  9,  8,  9,  9,  7,  9, 10, 10, 10,  7,\n",
      "       10,  4,  9,  9, 10,  9,  7, 10,  9,  8, 11, 12,  9, 12, 12],\n",
      "      dtype=int64), 'sizes': array([ 1,  3,  8, 18, 17, 11,  1, 13,  3, 13,  7,  1,  4], dtype=int64), 'random_state': None, 'score': 870}, {'predicted': array([2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1]), 'k': 3, 'ninti': 3, 'max_iter': 300, 'tol': 0.0001, 'verbose': 0, 'silhouette': 0.29260394950645463, 'centroids':           0         1         2         3         4         5         6  \\\n",
      "0 -0.667379 -0.776850 -0.362149 -0.631638 -0.477004 -0.537571 -0.497830   \n",
      "1  1.347915 -0.107534 -1.218250  2.962133  3.317539  2.937785  3.338479   \n",
      "2  0.488977  0.740261  0.464288  0.290708  0.109275  0.204885  0.126690   \n",
      "\n",
      "          7    8  \n",
      "0 -0.389701  0.0  \n",
      "1  0.855445  0.0  \n",
      "2  0.278551  0.0  , 'random_state': None, 'sizes': array([46,  5, 49], dtype=int64), 'score': 398}]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[{'max_iter': 200, 'verbose': 0, 'convergence_iter': 15, 'affinity': 'euclidean', 'k': 13, 'silhouette': 0.23085639527322566, 'predicted': array([ 1,  0,  1,  1,  5,  5,  2,  2,  2,  2,  3,  2,  2,  5,  2,  5,  3,\n",
      "        3,  3,  5,  4,  3,  3,  3,  3,  4,  4,  3,  3,  4,  3,  3,  3,  4,\n",
      "        3,  4,  3,  3,  5,  2,  5,  4,  4,  5,  5,  5,  5,  4,  4,  4,  9,\n",
      "        3, 12,  3,  4,  4,  4,  4,  4,  9,  7,  6,  7,  7,  7,  7,  7,  7,\n",
      "        7,  8,  9,  7,  7, 10,  9,  9,  8,  9,  9,  7,  9, 10, 10, 10,  7,\n",
      "       10,  4,  9,  9, 10,  9,  7, 10,  9,  8, 11, 12,  9, 12, 12],\n",
      "      dtype=int64), 'sizes': array([ 1,  3,  8, 18, 17, 11,  1, 13,  3, 13,  7,  1,  4], dtype=int64), 'random_state': None, 'score': 1162}, {'predicted': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1]), 'k': 3, 'ninti': 3, 'max_iter': 300, 'tol': 0.0001, 'verbose': 0, 'silhouette': 0.29098238154530665, 'centroids':           0         1         2         3         4         5         6  \\\n",
      "0  0.496872  0.762639  0.479203  0.265498  0.081108  0.163566  0.104172   \n",
      "1  1.141601 -0.145265 -1.057150  2.718579  3.008163  2.812854  2.983320   \n",
      "2 -0.667379 -0.776850 -0.362149 -0.631638 -0.477004 -0.537571 -0.497830   \n",
      "\n",
      "          7    8  \n",
      "0  0.299978  0.0  \n",
      "1  0.587877  0.0  \n",
      "2 -0.389701  0.0  , 'random_state': None, 'sizes': array([48,  6, 46], dtype=int64), 'score': 266}]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[{'max_iter': 200, 'verbose': 0, 'convergence_iter': 15, 'affinity': 'euclidean', 'k': 13, 'silhouette': 0.23085639527322566, 'predicted': array([ 1,  0,  1,  1,  5,  5,  2,  2,  2,  2,  3,  2,  2,  5,  2,  5,  3,\n",
      "        3,  3,  5,  4,  3,  3,  3,  3,  4,  4,  3,  3,  4,  3,  3,  3,  4,\n",
      "        3,  4,  3,  3,  5,  2,  5,  4,  4,  5,  5,  5,  5,  4,  4,  4,  9,\n",
      "        3, 12,  3,  4,  4,  4,  4,  4,  9,  7,  6,  7,  7,  7,  7,  7,  7,\n",
      "        7,  8,  9,  7,  7, 10,  9,  9,  8,  9,  9,  7,  9, 10, 10, 10,  7,\n",
      "       10,  4,  9,  9, 10,  9,  7, 10,  9,  8, 11, 12,  9, 12, 12],\n",
      "      dtype=int64), 'sizes': array([ 1,  3,  8, 18, 17, 11,  1, 13,  3, 13,  7,  1,  4], dtype=int64), 'random_state': None, 'score': 1030}, {'predicted': array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2]), 'k': 3, 'ninti': 3, 'max_iter': 300, 'tol': 0.0001, 'verbose': 0, 'silhouette': 0.2922670445292652, 'centroids':           0         1         2         3         4         5         6  \\\n",
      "0  0.511199  0.845176  0.498299  0.183213  0.021697  0.071244  0.046762   \n",
      "1 -0.682330 -0.862975 -0.382075 -0.545776 -0.415009 -0.441236 -0.437924   \n",
      "2  1.141601 -0.145265 -1.057150  2.718579  3.008163  2.812854  2.983320   \n",
      "\n",
      "          7    8  \n",
      "0  0.309712  0.0  \n",
      "1 -0.399857  0.0  \n",
      "2  0.587877  0.0  , 'random_state': None, 'sizes': array([48, 46,  6], dtype=int64), 'score': 3512}]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[{'max_iter': 200, 'verbose': 0, 'convergence_iter': 15, 'affinity': 'euclidean', 'k': 13, 'silhouette': 0.23085639527322566, 'predicted': array([ 1,  0,  1,  1,  5,  5,  2,  2,  2,  2,  3,  2,  2,  5,  2,  5,  3,\n",
      "        3,  3,  5,  4,  3,  3,  3,  3,  4,  4,  3,  3,  4,  3,  3,  3,  4,\n",
      "        3,  4,  3,  3,  5,  2,  5,  4,  4,  5,  5,  5,  5,  4,  4,  4,  9,\n",
      "        3, 12,  3,  4,  4,  4,  4,  4,  9,  7,  6,  7,  7,  7,  7,  7,  7,\n",
      "        7,  8,  9,  7,  7, 10,  9,  9,  8,  9,  9,  7,  9, 10, 10, 10,  7,\n",
      "       10,  4,  9,  9, 10,  9,  7, 10,  9,  8, 11, 12,  9, 12, 12],\n",
      "      dtype=int64), 'sizes': array([ 1,  3,  8, 18, 17, 11,  1, 13,  3, 13,  7,  1,  4], dtype=int64), 'random_state': None, 'score': 1142}, {'predicted': array([1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2]), 'k': 3, 'ninti': 3, 'max_iter': 300, 'tol': 0.0001, 'verbose': 0, 'silhouette': 0.29098238154530665, 'centroids':           0         1         2         3         4         5         6  \\\n",
      "0 -0.667379 -0.776850 -0.362149 -0.631638 -0.477004 -0.537571 -0.497830   \n",
      "1  0.496872  0.762639  0.479203  0.265498  0.081108  0.163566  0.104172   \n",
      "2  1.141601 -0.145265 -1.057150  2.718579  3.008163  2.812854  2.983320   \n",
      "\n",
      "          7    8  \n",
      "0 -0.389701  0.0  \n",
      "1  0.299978  0.0  \n",
      "2  0.587877  0.0  , 'random_state': None, 'sizes': array([46, 48,  6], dtype=int64), 'score': 366}]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "[{'max_iter': 200, 'verbose': 0, 'convergence_iter': 15, 'affinity': 'euclidean', 'k': 13, 'silhouette': 0.23085639527322566, 'predicted': array([ 1,  0,  1,  1,  5,  5,  2,  2,  2,  2,  3,  2,  2,  5,  2,  5,  3,\n",
      "        3,  3,  5,  4,  3,  3,  3,  3,  4,  4,  3,  3,  4,  3,  3,  3,  4,\n",
      "        3,  4,  3,  3,  5,  2,  5,  4,  4,  5,  5,  5,  5,  4,  4,  4,  9,\n",
      "        3, 12,  3,  4,  4,  4,  4,  4,  9,  7,  6,  7,  7,  7,  7,  7,  7,\n",
      "        7,  8,  9,  7,  7, 10,  9,  9,  8,  9,  9,  7,  9, 10, 10, 10,  7,\n",
      "       10,  4,  9,  9, 10,  9,  7, 10,  9,  8, 11, 12,  9, 12, 12],\n",
      "      dtype=int64), 'sizes': array([ 1,  3,  8, 18, 17, 11,  1, 13,  3, 13,  7,  1,  4], dtype=int64), 'random_state': None, 'score': 1160}, {'predicted': array([2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0]), 'k': 3, 'ninti': 3, 'max_iter': 300, 'tol': 0.0001, 'verbose': 0, 'silhouette': 0.29098238154530665, 'centroids':           0         1         2         3         4         5         6  \\\n",
      "0  1.141601 -0.145265 -1.057150  2.718579  3.008163  2.812854  2.983320   \n",
      "1 -0.667379 -0.776850 -0.362149 -0.631638 -0.477004 -0.537571 -0.497830   \n",
      "2  0.496872  0.762639  0.479203  0.265498  0.081108  0.163566  0.104172   \n",
      "\n",
      "          7    8  \n",
      "0  0.587877  0.0  \n",
      "1 -0.389701  0.0  \n",
      "2  0.299978  0.0  , 'random_state': None, 'sizes': array([ 6, 46, 48], dtype=int64), 'score': 4634}]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
